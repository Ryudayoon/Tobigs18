{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt1H5o-poBfF"
      },
      "source": [
        "# 1. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1X8mpttn-Vi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transfroms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWzCD1LRn-vT"
      },
      "source": [
        "# 2. Gpu or Cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YfkyOQioSBj"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' #GPU 사용 가능 여부 확인\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "print(device + \" is available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ_MTPYyoUbo"
      },
      "source": [
        "# 3. Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQee8cNioUjL"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDVGW05NoUri"
      },
      "source": [
        "# 4. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn6FU1proUxO"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset을 가져오고 transforms를 통해 이를 Tensor 객체로 가공. Train set과 Test set으로 구성\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() \n",
        "    ])\n",
        ")\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor()\n",
        "    ])\n",
        ")\n",
        "\n",
        "# DataLoader를 통해 dataset의 전체 데이터를 batch size로 slice해 실제로 학습할 때 이용할 수 있는 형태로 생성\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        "\n",
        "examples = enumerate(train_set)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "example_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccwsHldkoU3w"
      },
      "source": [
        "# 5. Model Define"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN9BQMZJoU8b"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self): \n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # in_channels: 1, out_channels: 10, kernel_size: 5, stride: 1, padding: 0인 convolution 연산\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # in_channels: 10, out_channels: 20, kernel_size: 5, stride: 1, padding: 0인 convolution 연산\n",
        "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # p: 0.25로 학습시 dropout 진행, inplace: False\n",
        "        self.mp = nn.MaxPool2d(2) # kernel_size=2로 Max pool 진행\n",
        "        self.fc1 = nn.Linear(320,100) # in_features: 320, out_features: 100\n",
        "        self.fc2 = nn.Linear(100,10) # in_features: 100, out_features: 10 \n",
        "\n",
        "  def forward(self, x):\n",
        "        x = F.relu(self.mp(self.conv1(x))) #relu 연산\n",
        "        x = F.relu(self.mp(self.conv2(x))) \n",
        "        x = self.drop2D(x) \n",
        "        x = x.view(x.size(0), -1) # flatten\n",
        "        x = self.fc1(x) \n",
        "        x = self.fc2(x) \n",
        "        return F.log_softmax(x) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35KJ9PP9oVCI"
      },
      "source": [
        "# 6. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aJMrF54oVGV"
      },
      "outputs": [],
      "source": [
        "model = ConvNet().to(device) \n",
        "criterion = nn.CrossEntropyLoss().to(device) # CrossEntropyLoss을 연산\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # Adam optimizer 사용\n",
        " \n",
        "for epoch in range(epochs): \n",
        "    avg_cost = 0 #초기화\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad() #모든 model의 gradient 값을 0으로 설정 - 초기화 이유 : backward()를 통해 구한 gradient가 누적됨\n",
        "        hypothesis = model(data)\n",
        "        cost = criterion(hypothesis, target) \n",
        "        cost.backward() #gradient 계산\n",
        "        optimizer.step() #파라미터 갱신\n",
        "        avg_cost += cost / len(train_loader) \n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bxJSYFToVLy"
      },
      "source": [
        "# 7. Evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yzDsZ9roVQM"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "with torch.no_grad(): \n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        out = model(data)\n",
        "        preds = torch.max(out.data, 1)[1] # 가장 큰 값을 예측값으로 설정\n",
        "        total += len(target) \n",
        "        correct += (preds==target).sum().item() \n",
        "        \n",
        "    print('Test Accuracy: ', 100.*correct/total, '%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Framework_PyTorch_Dayoon.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}